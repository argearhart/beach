{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook\n",
    "This notebook aims to analyze and visualize combined data as well as implement additional features from the project requirements.  \n",
    "US Stations is from US Linear Relative Sea Level Trends.  \n",
    "Storms is from the Natural Disasters data and contains the US storm information.  \n",
    "Beach contains the beach project data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt', download_dir='/nltk_data')\n",
    "              \n",
    "nltk.download('stopwords', download_dir='/nltk_data')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "usstations = pd.read_excel('data/usstations.xlsx')\n",
    "beach = pd.read_excel('data/beach_data.xlsx')\n",
    "storms = pd.read_excel('data/cleanusstorms.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group data by the region column and aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationsgroup = usstations.groupby('Region').size().reset_index(name='counts')\n",
    "stationsgroup = stationsgroup.sort_values('counts', ascending=False)\n",
    "stationsgroup = stationsgroup.head(10)\n",
    "print(stationsgroup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beachgroup = beach.groupby('Region').size().reset_index(name='counts')\n",
    "beachgroup = beachgroup.sort_values('counts', ascending=False)\n",
    "beachgroup = beachgroup.head(10)\n",
    "print(beachgroup)\n",
    "\n",
    "# No projects are listed for Hawaii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stormsgroup = storms.groupby('Region').size().reset_index(name='counts')\n",
    "stormsgroup = stormsgroup.sort_values('counts', ascending=False)\n",
    "stormsgroup = stormsgroup.head(10)\n",
    "print(stormsgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.merge(usstations, beach, on='State', how='outer')\n",
    "\n",
    "df = pd.merge(merge_df, storms, on='State', how='outer')\n",
    "print(df.head(5))\n",
    "print(df.columns)\n",
    "df.to_csv('data/merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate MSL trends by state\n",
    "avg_msl_trends = usstations.groupby('State')['MSL Trends mm per yr'].mean()\n",
    "\n",
    "# Aggregate beach nourishment volume by state\n",
    "total_nourishment_volume = beach.groupby('State')['Volume (CY)'].sum()\n",
    "\n",
    "sealevel_Beachvolume = pd.DataFrame({\n",
    "    'Avg MSL Trend (mm/yr)': avg_msl_trends,\n",
    "    'Total Nourishment Volume (CY)': total_nourishment_volume\n",
    "}).reset_index()\n",
    "print(sealevel_Beachvolume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sealevel_Beachvolume['Total Nourishment Volume (CY)'] = sealevel_Beachvolume['Total Nourishment Volume (CY)'].fillna(0)\n",
    "sealevel_Beachvolume = sealevel_Beachvolume.dropna(subset=['Avg MSL Trend (mm/yr)', 'Total Nourishment Volume (CY)'])\n",
    "\n",
    "\n",
    "\n",
    "fig = px.scatter(sealevel_Beachvolume,\n",
    "                 x='Avg MSL Trend (mm/yr)',\n",
    "                 y='Total Nourishment Volume (CY)',\n",
    "                 size='Total Nourishment Volume (CY)',\n",
    "                 hover_name='State',  \n",
    "                 title='Relationship between MSL Trends and Beach Nourishment Volume by State')\n",
    "\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.update_layout(xaxis_title='Avg MSL Trend (mm per year)',\n",
    "                  yaxis_title='Total Nourishment Volume (Cubic Yards)',\n",
    "                  xaxis_showgrid=False,\n",
    "                  yaxis_showgrid=False)\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: Summarize text from separate documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('data/merged_data.csv', low_memory=False) #Suppress low memory warning\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)  # Returning a string of processed tokens\n",
    "\n",
    "# Process all text columns and concatenate them into a new column\n",
    "merged_df['CombinedText'] = merged_df.apply(lambda row: ' '.join([preprocess_text(str(row[col])) for col in df.columns if pd.api.types.is_string_dtype(df[col])]), axis=1)\n",
    "\n",
    "# Tokenize the combined text for frequency analysis\n",
    "all_tokens = word_tokenize(' '.join(merged_df['CombinedText'].tolist()))\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = Counter(all_tokens)\n",
    "\n",
    "# Convert to DataFrame for the top N words\n",
    "df_freq = pd.DataFrame(word_freq.most_common(10), columns=['Word', 'Frequency'])\n",
    "\n",
    "print(df_freq)\n",
    "\n",
    "# Exporting the frequencies to CSV\n",
    "df_freq.to_csv('data/word_frequencies.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
